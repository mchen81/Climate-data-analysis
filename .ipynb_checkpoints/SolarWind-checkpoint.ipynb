{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import geohash\n",
    "\n",
    "def parseLine(line):\n",
    "    variables = line.split(\"\\t\")\n",
    "    try:\n",
    "        lat = float(variables[1])\n",
    "        lon = float(variables[2])\n",
    "        wind_speed = float(variables[17])\n",
    "        cloud_cover = float(variables[12])\n",
    "        #Energy is proportional to the third power of wind speed.\n",
    "        #At here, we define wind_energy_factor = wind_speed * wind_speed * wind_speed\n",
    "        wind_energy_factor = wind_speed * wind_speed * wind_speed;\n",
    "        \n",
    "        gh = geohash.encode(lat, lon)\n",
    "        return (gh, wind_energy_factor, cloud_cover)\n",
    "    except:\n",
    "        return ('', 0, 0)\n",
    "    \n",
    "#text_fileTT = spark.read.load('hdfs://orion11:21001/3hr_sample/sampled_2015/*', format='csv', sep='\\t', inferSchema=True, header=True)\n",
    "text_file = sc.textFile(\"hdfs://orion11:21001/3hr/2018/*\")\n",
    "\n",
    "# (GeoHash, wind_energy_factor, cloud_cover)\n",
    "parsed_data = text_file \\\n",
    "    .map(lambda line: parseLine(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we sort the geohashs by their cumulative wind_energy_factor to find out top3 wind farm\n",
    "wind_data_top3 = parsed_data.map(lambda data: (data[0][0: 4], data[1])) \\\n",
    "    .reduceByKey(lambda a, b: a + b) \\\n",
    "    .sortBy(lambda x: x[1], False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dxx7', 30610549.790516056),\n",
       " ('dxwe', 30515688.971412513),\n",
       " ('dxwk', 30197750.69924766)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "top 3 best wind places\n",
    "dxx7\n",
    "dxwe\n",
    "dxwk\n",
    "'''\n",
    "wind_data_top3.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we sort the geohashs by their cumulative cloud_cover in decreasing order to find out top3 solar farm\n",
    "solar_data_top3 = parsed_data.map(lambda data: (data[0][0: 4], data[2])) \\\n",
    "    .reduceByKey(lambda a, b: a + b) \\\n",
    "    .sortBy(lambda x: x[1], True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('', 0), ('dd93', 26282.0), ('d6bm', 27174.0), ('dd9g', 27642.0)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "top 3 best solar energy places\n",
    "dd93\n",
    "d6bm\n",
    "dd9g\n",
    "'''\n",
    "solar_data_top3.take(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('', 5920975.497173111)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Average cumulative wind_energy_factor in all geohashs\n",
    "avg_wind = wind_data_top3.map(lambda data: (\"\", data[1])) \\\n",
    "    .mapValues(lambda x: (x, 1)) \\\n",
    "    .reduceByKey(lambda x, y: (x[0] + y[0], x[1]+y[1])) \\\n",
    "    .mapValues(lambda v: (v[0] / v[1]))\n",
    "\n",
    "avg_wind.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Average cumulative cloud_cover in all geohashs\n",
    "avg_solar = solar_data_top3 \\\n",
    "    .map(lambda data: (\"\", data[1])) \\\n",
    "    .filter(lambda data: data[0] != \"\") \\\n",
    "    .mapValues(lambda x: (x, 1)) \\\n",
    "    .reduceByKey(lambda x, y: (x[0] + y[0], x[1]+y[1])) \\\n",
    "    .mapValues(lambda v: (v[0] / v[1]))\n",
    "\n",
    "avg_solar.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#As you can see, the average cumulative wind_energy_factor in all geohashs is around 5920975\n",
    "#the average cumulative cloud_cover in all geohashs is around 319382\n",
    "#So we set the factor f = 185, we will us it later.\n",
    "#Then, average_cumulative_wind_energy_factor - (f * average_cumulative_cloud_cover) = 0\n",
    "\n",
    "#To answer the question: Locate the top 3 places for solar + wind farm.\n",
    "#We define combine_energy_factor = wind_energy_factor - (f * cloud_cover).\n",
    "#Then, try to find out the top3 geohashs with biggest cumulative_combine_energy_factor. \n",
    "#We regard them as top 3 places for solar + wind farm. \n",
    "#Because these places has big cumulative_wind_energy_factor with small cumulative_cloud_cover,\n",
    "#which means they have more wind energy with less cloud cover.\n",
    "\n",
    "\n",
    "import geohash\n",
    "\n",
    "def parseLine(line):\n",
    "    variables = line.split(\"\\t\")\n",
    "    try:\n",
    "        lat = float(variables[1])\n",
    "        lon = float(variables[2])\n",
    "        wind_speed = float(variables[17])\n",
    "        cloud_cover = float(variables[12])\n",
    "        #Energy is proportional to the third power of wind speed.\n",
    "        #At here, we define wind_energy_factor = wind_speed * wind_speed * wind_speed\n",
    "        wind_energy_factor = wind_speed * wind_speed * wind_speed;\n",
    "        \n",
    "        gh = geohash.encode(lat, lon)\n",
    "        return (gh, wind_energy_factor - 185 * cloud_cover)\n",
    "    except:\n",
    "        return ('', 0)\n",
    "    \n",
    "#text_fileTT = spark.read.load('hdfs://orion11:21001/3hr_sample/sampled_2015/*', format='csv', sep='\\t', inferSchema=True, header=True)\n",
    "text_file = sc.textFile(\"hdfs://orion11:21001/3hr/2018/*\")\n",
    "\n",
    "# (GeoHash, wind_energy_factor, cloud_cover)\n",
    "parsed_data = text_file \\\n",
    "    .map(lambda line: parseLine(line))\n",
    "\n",
    "\n",
    "wind_solar_top3 = parsed_data.map(lambda data: (data[0][0: 4], data[1])) \\\n",
    "    .reduceByKey(lambda a, b: a + b) \\\n",
    "    .sortBy(lambda x: x[1], False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('', 0),\n",
       " ('d6bm', -3482372.517710123),\n",
       " ('dd93', -3613794.311647366),\n",
       " ('9mr4', -3808428.2687501083)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Here is the top3 solar + wind farm places in our view\n",
    "d6bm\n",
    "dd93\n",
    "9mr4\n",
    "'''\n",
    "\n",
    "wind_solar_top3.take(4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
