{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomaly Detector\n",
    "\n",
    "This process takes a streaming data of NAM and gives an anomalous score to each row \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy  for detecting anomaly\n",
    "The most common approach to detect anomalies in statistics is to evaluate a mean and standard deviation. And the standard deviation equals the square root of the sample variance. An anomalous value is considered when it overseeds/below the mean plus/subtract 3 times of standard deviation.  \n",
    "\n",
    "Based on this rule, we evaluated each incoming value as normal or anomaly.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def is_anomalous(value, mean, sampleVarience):\n",
    "    standard_deviation = math.sqrt(sampleVarience)\n",
    "    upper_bound = mean + (3 * standard_deviation)\n",
    "    lower_bound = mean - (3 * standard_deviation)\n",
    "    if(value > upper_bound or value < lower_bound):\n",
    "        return True\n",
    "    else: \n",
    "        return False\n",
    "\n",
    "def update_mean(newValue, count, existingAggregate):   \n",
    "    count = count + 1\n",
    "    (mean, M2) = existingAggregate\n",
    "    delta = newValue - mean\n",
    "    mean += delta / count\n",
    "    delta2 = newValue - mean\n",
    "    M2 += delta * delta2\n",
    "    return (mean, M2)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy for grouping area\n",
    "\n",
    "The location index from the NAM is latitude and longitude, it would be hard work to distinguish which points are in the same area. An approach is to transform the latitude and longitude to geo-hash. The longer the hashed value, the accurate result we will get. However, we are wondering there is not enough data for a specific point to have a fair mean and standard deviation, so we decided to take 6 bits from the geo-hash. In other words, the position has the first 6 identical geo-hash codes that will be considered as the same area. \n",
    "\n",
    "The *gh_features_dict* is a dictionary storing the 6-code geo-hash as key, and the value is a tuple of each column's mean and M2(i.e. squared distance from the mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "gh_features_dict = {} # wxyz -> (count, pmw, ps, pt, hum, sd, ts, tt, prec, vs, vis) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anomaly  Evaluation\n",
    "\n",
    "We take 10 columns from the NAM, which are pressure_maximum_wind, pressure_surface, pressure_tropopause, humidity, snow_depth_surface, temperature_surface, temperature_tropopause, precipitation, vegetation_surface, and visibility.   \n",
    "When one of the value is considered as anomaly, the row recieves 10 anomaly points in terms of anomalous score. Eventullay, it will output the row with the anomalous score. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import geohash\n",
    "\n",
    "def is_float(value):\n",
    "    try:\n",
    "        float(value)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def calcuate_anomaly(line):\n",
    "    \n",
    "    # check the if incoming line is valid \n",
    "    if line.startswith('1_'): # header\n",
    "        return ''\n",
    "    \n",
    "    variables = line.split(\"\\t\")\n",
    "    \n",
    "    if(len(variables) < 16): # Could be invalid text\n",
    "        return ''\n",
    "    \n",
    "    # If a value cannot be converted from a stirng no a number, \n",
    "    # we give it a negative value so it would be considred as \n",
    "    # anomaly but not be counted to the mean\n",
    "    for i in range(len(variables)):\n",
    "        if is_float(variables[i]) == False:\n",
    "            variables[i] = '-999999'\n",
    "    \n",
    "    # convert each column\n",
    "    milliseconds = int(variables[0])\n",
    "    dt = datetime.datetime.fromtimestamp(milliseconds/1000.0)\n",
    "    lat = float(variables[1])\n",
    "    lon = float(variables[2])\n",
    "    pressure_maximum_wind = float(variables[5])\n",
    "    pressure_surface = float(variables[6])\n",
    "    pressure_tropopause = float(variables[7])\n",
    "    humidity = float(variables[8])\n",
    "    snow_depth_surface = float(variables[9])\n",
    "    temperature_surface = float(variables[10])\n",
    "    temperature_tropopause = float(variables[11])\n",
    "    precipitation = float(variables[13])\n",
    "    vegetation_surface = float(variables[14])                          \n",
    "    visibility = float(variables[15])\n",
    "    \n",
    "    # put features to a tuple, the 1 in the first index means count\n",
    "    newValues = [1, pressure_maximum_wind, pressure_surface, pressure_tropopause, humidity, \\\n",
    "                snow_depth_surface, temperature_surface, temperature_tropopause, precipitation, \\\n",
    "                vegetation_surface, visibility]\n",
    "    try:\n",
    "        # get 6-bit geo-hashed \n",
    "        gh = geohash.encode(lat, lon)[0:6]\n",
    "    except:\n",
    "        return ''\n",
    "    \n",
    "    # define the anomalous_score\n",
    "    anomalous_score = 0\n",
    "    if gh in gh_features_dict: # \n",
    "        features = gh_features_dict[gh]\n",
    "        cnt = features[0]\n",
    "        if(cnt >= 3): # To get a fair result, the calculation will not be invoked until an area has at least 3 records \n",
    "            for i in range(1, 10):\n",
    "                (mean, M2) = features[i]\n",
    "                sampleVariance =  M2 / (cnt - 1)\n",
    "                anomalous_score += 10 if is_anomalous(newValues[i], mean, sampleVariance) else 0\n",
    "    else : # if no geo code in the gh_features_dict, we simply give it a empty record\n",
    "        features = [0, (0,0), (0,0), (0,0), (0,0), (0,0), (0,0), (0,0), (0,0), (0,0), (0,0)]\n",
    "    \n",
    "    cnt = features[0]\n",
    "    features[0] = cnt + 1\n",
    "    \n",
    "    # No matter the data is normal or not, it will be counted into the location's mean and M2.\n",
    "    # However, a negative value will not being counted because the value is invalid(from the pre-process)\n",
    "    for i in range(1, 10) : \n",
    "        if newValues[i] < 0 :\n",
    "            continue\n",
    "        features[i]  = update_mean(newValues[i], cnt, features[i])\n",
    "    gh_features_dict[gh] = features\n",
    "    \n",
    "    # In the end, we return the original line with the anomalous score\n",
    "    return line[0:len(line)-1] + '\\t' + str(anomalous_score) + '\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start processing\n",
    "Now let's open a socket waiting for the data emitter, and output to a file(anomaly-detector-output.txt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up exisitng data\n",
    "output_file_name = \"anomaly-detector-output.txt\"\n",
    "f = open(output_file_name, \"w\")\n",
    "f.write(\"\")\n",
    "f.close()\n",
    "\n",
    "def write_rdd_to_file(rdd):\n",
    "    f = open(output_file_name, \"a\")\n",
    "    for line in rdd:\n",
    "        if not line or len(line) == 0:\n",
    "            continue\n",
    "        f.write(line)\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "ssc = StreamingContext(sc, 0.5)\n",
    "ssc.checkpoint(\"checkpoint\")\n",
    "lines = ssc.socketTextStream(\"orion03\", 12889)\n",
    "processed = lines.map(lambda line: calcuate_anomaly(line))\n",
    "processed.foreachRDD(lambda rdd: write_rdd_to_file(rdd.collect()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.stop(stopSparkContext=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
